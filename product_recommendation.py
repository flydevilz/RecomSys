# -*- coding: utf-8 -*-
"""Product Recommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U0P_MwyK1cS0QtXNLnHHnEBlYgUSd5wu

# Prepare

This cell imports a variety of essential libraries and modules that will be used throughout the code for data manipulation, visualization, text processing, and machine learning.

1. `gdown`: Used to download files from Google Drive using a direct link.
2. `numpy`: A fundamental package for numerical computing in Python, providing support for large multi-dimensional arrays and matrices.
3. `pandas`: A powerful data manipulation and analysis library that provides data structures like DataFrames.
4. `matplotlib.pyplot`: A plotting library used for creating static, animated, and interactive visualizations in Python.
5. `seaborn`: A statistical data visualization library built on top of Matplotlib that provides a high-level interface for drawing attractive graphs.
6. `string`: A module providing various string operations.
7. `random`: A module that implements pseudo-random number generators for various distributions.
8. `re`: A module providing support for regular expressions in Python, used for string matching and manipulation.
9. `sklearn.feature_extraction.text.TfidfVectorizer`: A class that converts a collection of raw documents into a matrix of TF-IDF features.
10. `sklearn.metrics.pairwise.cosine_similarity, linear_kernel`: Functions for computing similarity between vectors, particularly useful in text processing and recommendation systems.
11. `zipfile.ZipFile`: A module that allows working with ZIP archives.
12. `tensorflow as tf`: TensorFlow is a deep learning framework for building and deploying machine learning models.
13. `tensorflow.keras`: A high-level neural networks API, part of TensorFlow, used for building and training models.
14. `tensorflow.keras.layers`: A module in Keras that contains various layers to build neural networks.
15. `pathlib.Path`: A module that offers classes representing filesystem paths with semantics appropriate for different operating systems.
"""

import gdown
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import string
import random
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity, linear_kernel
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path

"""In this section, the dataset are downloaded from Google Drive and Kaggle. The steps are below:
1. Use `gdown` to download the Kaggle API key from Google Drive.
2. Install the Kaggle API.
3. Set up Kaggle configuration.
4. Download the dataset from Kaggle. (dataset link: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)
5. Unzip the downloaded dataset.
"""

api = 'https://drive.google.com/uc?id=1HwbZemgag2jcb9QcEaJ1KcOajF8_xW36'

gdown.download(api, 'kaggle.json', quiet=False)

!pip install kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d olistbr/brazilian-ecommerce
!unzip /content/brazilian-ecommerce.zip

"""# Data Understanding

In this section, each dataset is loaded and unnecessary columns that are not needed for further analysis are removed. After that, each dataframe structures is displayed, including the number of entries, column names, data types, and the number of non-null values in each column.

## Product Table
"""

product_item = pd.read_csv('/content/olist_products_dataset.csv')
product_item = product_item.drop(columns=['product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm'])
product_item.head()

product_item.info()

"""## Bridging ID Table"""

bridging_product_and_orderid = pd.read_csv('/content/olist_order_items_dataset.csv')
bridging_product_and_orderid = bridging_product_and_orderid.drop(columns=['shipping_limit_date', 'price', 'freight_value','order_item_id'])
bridging_product_and_orderid.head()

bridging_product_and_orderid.info()

"""## Seller Data"""

bridging_orderid_and_sellerid = pd.read_csv('/content/olist_sellers_dataset.csv')

bridging_orderid_and_sellerid.info()

"""In this section, random store names are generated and mapped to each unique seller in the dataset. The process includes:
1. **Generate Unique Store Names**: A function is defined to create random store names for each unique `seller_id`.
2. **Map Store Names**: Store names are assigned to each seller in the DataFrame using a dictionary.
3. **Clean the DataFrame**: Drops the `seller_zip_code_prefix` column, as it is not needed for further analysis.

"""

unique_seller_ids = bridging_orderid_and_sellerid['seller_id'].unique()

def generate_random_store_name(length=8, random_state=42):
    random_string = ''.join(random.choices(string.ascii_lowercase, k=length))
    return f'{random_string} store'

# Create a dictionary mapping seller_id to a random store name
store_names = {seller_id: generate_random_store_name() for seller_id in unique_seller_ids}

# Map the store names to the original dataframe
bridging_orderid_and_sellerid['store_name'] = bridging_orderid_and_sellerid['seller_id'].map(store_names)

# Drop irrelevant columns
bridging_orderid_and_sellerid = bridging_orderid_and_sellerid.drop(columns=['seller_zip_code_prefix'])

bridging_orderid_and_sellerid.head()

"""## Product purchase table"""

bridging_product_and_custid = pd.read_csv('/content/olist_orders_dataset.csv')
bridging_product_and_custid = bridging_product_and_custid.drop(columns=['order_status', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date','order_purchase_timestamp'])
bridging_product_and_custid.head()

bridging_product_and_custid.info()

"""## Review Table"""

reviewid_orderid = pd.read_csv('/content/olist_order_reviews_dataset.csv')
reviewid_orderid = reviewid_orderid.drop(columns=['review_comment_title', 'review_comment_message', 'review_creation_date', 'review_answer_timestamp'])
reviewid_orderid.head()

reviewid_orderid.info()

"""## All Table Summarise

In this section, the code calculates and displays the unique counts of various entities in the dataset
"""

print('Jumlah produk : ', len(product_item.product_category_name.unique()))
print('Jumlah customer : ', len(bridging_product_and_custid.customer_id.unique()))
print('Jumlah order : ', len(bridging_product_and_orderid.order_id.unique()))
print('Jumlah review : ', len(reviewid_orderid.review_id.unique()))
print('Jumlah seller : ', len(bridging_orderid_and_sellerid.seller_id.unique()))

"""# Data Preprocessing

## Table Merge

In this section, multiple DataFrames are merged to create a comprehensive dataset:
1. **Merge Product and Order Data**: Merges the `product_item` DataFrame with `bridging_product_and_orderid` on `product_id`.
2. **Merge with Seller Data**: Merges the resulting DataFrame with `bridging_orderid_and_sellerid` on `seller_id`.
3. **Merge with Customer Data**: Merges the resulting DataFrame with `bridging_product_and_custid` on `order_id`.
4. **Merge with Review Data**: Merges the final DataFrame with `reviewid_orderid` on `order_id`.
5. **Display the Data**: Shows the first few rows of the merged DataFrame.
"""

a = pd.merge(product_item, bridging_product_and_orderid, on='product_id', how='outer')
b = pd.merge(a, bridging_orderid_and_sellerid, on='seller_id', how='outer')
c= pd.merge(b, bridging_product_and_custid, on='order_id', how='outer')
d = pd.merge(c, reviewid_orderid, on='order_id', how='outer')
d.head()

d.info()

d.isnull().sum()

"""## Table Cleaning

In this section, the dataset is cleaned and filtered to focus on specific data:
1. **Remove Rows with Missing `review_score`**: Drops rows where the `review_score` is missing.
2. **Remove Rows with Missing `seller_id`**: Drops rows where the `seller_id` is missing.
3. **Remove Rows with Missing `product_category_name`**: Drops rows where the `product_category_name` is missing.
4. **Display DataFrame Structure**: Shows the structure of the cleaned DataFrame, including the number of entries and data types.
"""

f = d.dropna(subset=['review_score'])
g = f.dropna(subset=['seller_id'])
h = g.dropna(subset=['product_category_name'])

h.info()

"""Filters the dataset to include only rows where the `seller_state` is 'MG', creating a smaller, focused dataset."""

k = h[(h['seller_state'] == 'MG')]

"""## Data Integrity Check

In this section, the dataset is further cleaned to ensure consistency in city names:
1. **Identify Unique City Names**: Lists all unique city names to check for errors or typos.
2. **Correct City Name Typos**: Applies corrections to known misspelled city names using a predefined dictionary.
3. **Remove Incorrect Entries**: Filters out rows where `seller_city` contains incorrect entries such as 'minas gerais' and 'castro pires'.
4. **Drop Duplicate Rows**: Removes any duplicate rows to ensure data integrity.
"""

# Check if there is error in spelling the name of the city
k['seller_city'].unique()

# Fix typo city name
corrections = {
    'barbacena/ minas gerais' : 'barbacena',
    'belo horizont' : 'belo horizonte'
}
k['seller_city'] = k['seller_city'].replace(corrections)

#  Remove incorrect entries
k = k[(k['seller_city'] != 'minas gerais') & (k['seller_city'] != 'castro pires')]
#  Drop duplicate entries
k = k.drop_duplicates()

"""## Summarise on State MG

This code calculates and displays the unique counts of various entities in the cleaned and filtered dataset of MG State.
"""

print('Number of products : ', len(k.product_category_name.unique()))
print('Number of customers : ', len(k.customer_id.unique()))
print('Number of cities : ', len(k.seller_city.unique()))
print('Number of orders : ', len(k.order_id.unique()))
print('Number of reviews : ', len(k.review_id.unique()))
print('Number of sellers : ', len(k.seller_id.unique()))

"""# Exploratory Data Analysis (EDA)

## Top product categories

This section visualizes the top 10 product categories based on their frequency in the dataset. The `value_counts()` function is used to count the occurrences of each product category, and the top 10 categories are selected for the plot. The bar chart below displays these categories along with their respective counts.
"""

top_categories = k['product_category_name'].value_counts().head(10)

# Plot the top product categories
plt.figure()
top_categories.plot(kind='bar', color='skyblue')
plt.title('Top 10 Product Categories by Count')
plt.xlabel('Product Category')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.show()

"""## Rating Statistical Summary"""

k['review_score'].describe()

"""## Count Of Each Review Score"""

# Count the number of occurrences for each review score
rating_counts = k['review_score'].value_counts().sort_index()

# Plot the count of each review score
plt.figure()
rating_counts.plot(kind='bar', color='lightcoral')
plt.title('Count of Each Review Score')
plt.xlabel('Review Score')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.show()

"""## Highest Rated Product

This section analyzes and visualizes the top 10 highest-rated products based on their review scores. The `groupby` function is utilized to aggregate the review scores by product category, and the top 10 products are selected based on their total review scores.
"""

review_score_by_product = k.groupby('product_category_name').sum()

top_10_product = review_score_by_product.sort_values(by='review_score', ascending=False).head(10)

plt.figure()
top_10_product.plot(kind='bar', color='green')
plt.title('Top 10 Highest-Rated Product')
plt.xlabel('Product')
plt.ylabel('Product Review Score')
plt.xticks(rotation=45, ha='right')
plt.show()

"""## Average review score by city

This section examines the distribution of review scores in the dataset. The `value_counts()` function is used to count the occurrences of each review score, which is then sorted by the score value to facilitate a clear understanding of the distribution.
"""

# Calculate the average review score by city
review_score_by_city = k.groupby('seller_city')['review_score'].sum().sort_values(ascending=False)

# Plot the top 10 highest-rated cities for better visualization
top_10_cities = review_score_by_city.head(10)

plt.figure()
top_10_cities.plot(kind='bar', color='lightblue')
plt.title('Top 10 Highest-Rated Cities by Average Review Score')
plt.xlabel('City')
plt.ylabel('Average Review Score')
plt.xticks(rotation=45, ha='right')
plt.show()

"""## Top Store

This section focuses on identifying and visualizing the top 10 highest-rated stores based on their cumulative review scores. The `groupby` function is used to aggregate review scores by store name, and the stores with the highest total scores are selected.
"""

review_score_by_store = k.groupby('store_name').sum()

top_10_stores = review_score_by_store.sort_values(by='review_score', ascending=False).head(10)

plt.figure()
top_10_stores.plot(kind='bar', color='green')
plt.title('Top 10 Highest-Rated Store')
plt.xlabel('Store Name')
plt.ylabel('Store Review Score')
plt.xticks(rotation=45, ha='right')
plt.show()

"""# Content Based Filtering

## Data Preparation

In this section, the dataset is prepared for content-based filtering:
1. **Drop Unnecessary Columns**: Removes columns that are not needed for the content-based filtering process, such as `product_id`, `product_category_name`, and others.
2. **Remove Duplicates**: Drops any duplicate rows to ensure that each entry is unique.
3. **Reset Index**: Resets the index of the DataFrame after dropping duplicates to ensure sequential indexing.
"""

l = k.drop(columns=['product_id', 'product_category_name', 'order_id', 'seller_id', 'seller_state', 'customer_id', 'review_id', 'review_score'])
l= l.drop_duplicates().reset_index(drop=True)

l.head()

"""## Feature Extraction

In this section, a TF-IDF Vectorizer is applied to the `seller_city` column to transform the city names into a numerical matrix:
1. **Initialize TF-IDF Vectorizer**: Creates an instance of the `TfidfVectorizer`, which converts text data into a matrix of TF-IDF features.
2. **Transform City Names**: The `fit_transform` method is used to learn the vocabulary and create a TF-IDF matrix based on the `seller_city` values.
3. **Check Matrix Shape**: Prints the shape of the resulting matrix, showing the number of rows (cities) and columns (features).
"""

tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(l['seller_city'])

# Check the shape of the resulting matrix
print(tfidf_matrix.shape)

"""In this section, the code computes and visualizes the cosine similarity between different seller cities:
1. **Compute Cosine Similarity**: Uses the `linear_kernel` function to calculate the cosine similarity between the TF-IDF vectors of `seller_city`, resulting in a similarity matrix.
2. **Create DataFrame**: Converts the similarity matrix into a DataFrame, with city names as both the row and column labels for easy interpretation.
3. **Output the Shape**: Prints the shape of the DataFrame to show the number of cities and the dimensions of the similarity matrix.
4. **View Sample of Similarity Matrix**: Displays a random sample of the similarity matrix to get a glimpse of the calculated similarities between different cities.
"""

# Compute the cosine similarity matrix
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

# Create a DataFrame from the cosine similarity matrix with city names as both rows and columns
cosine_sim_df = pd.DataFrame(cosine_sim, index=l['seller_city'], columns=l['seller_city'])

# Output the shape of the DataFrame
print('Shape:', cosine_sim_df.shape)

# View a sample of the similarity matrix
print(cosine_sim_df.sample(5, axis=1).sample(10, axis=0))

"""## Getting Recommendation

In this section, the code sets up a mechanism to recommend stores based on the similarity of their cities:

1. **Create Mapping of Store Names to Indices**:
   - A `Series` is created to map each `store_name` to its corresponding index in the DataFrame. This helps quickly locate a store's index using its name.

2. **Define Recommendation Function**:
   - A function `get_recommendations` is defined to retrieve store recommendations based on city similarity.
   - **Find Store Index**: The function begins by finding the index of the given `store_name` using the `indices` mapping.
   - **Compute Similarity Scores**: It then retrieves the pairwise similarity scores between the selected store and all other stores.
   - **Sort and Rank Stores**: The stores are sorted by their similarity scores in descending order.
   - **Select Top Recommendations**: The function returns the top 10 stores that are most similar to the selected store based on city similarity, excluding the store itself.

This setup allows users to get recommendations of stores that are located in cities similar to the one where the selected store is located.
"""

# Create a series to map store_name to the index
indices = pd.Series(l.index, index= l['store_name']).drop_duplicates()

# Function to get recommendations based on city similarity
def get_recommendations(store_name, cosine_sim=cosine_sim):
    idx = indices[store_name]

    # Get the pairwise similarity scores of all stores with that store
    sim_scores = cosine_sim[idx]

    # Ensure sim_scores is a 1D array (flatten if necessary)
    if len(sim_scores.shape) > 1:
        sim_scores = sim_scores.flatten()

    # Pair store indices with similarity scores
    sim_scores = list(enumerate(sim_scores))

    # Sort the stores based on the similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get the scores of the 10 most similar stores
    sim_scores = sim_scores[1:11]

    # Get the store indices
    store_indices = [i[0] for i in sim_scores]

    # Return the top 10 most similar stores based on city similarity
    return l['store_name'].iloc[store_indices]

"""The `get_recommendations` function is called with the store name to generate recommendations."""

# Get the store name from the user
user_input = input("Enter the store name: ")

# Get the city name for the input store
input_city = l.loc[l['store_name'] == user_input, 'seller_city'].values[0]

# Call the get_recommendations function with the user input
recommended_stores = get_recommendations(user_input)

# Display the recommendations with the city name
print(f"Top 10 recommended stores similar to '{user_input}' in {input_city}:")
for i, store in enumerate(recommended_stores, 1):
    # Get the city name for each recommended store
    store_city = l.loc[l['store_name'] == store, 'seller_city'].values[0]
    print(f"{i}. {store}")

"""## Evaluation

In this section, the performance of the Content-Based Filtering model is evauated using the Precision metric. Precision measures the proportion of relevant items among the recommended items, indicating how accurate the recommendations are.
"""

store_name = user_input
recommended_stores = get_recommendations(store_name)

# Define Relevant Items
relevant_stores = l[l['store_name'] == store_name]['seller_city'].unique()
relevant_recommendations = recommended_stores[recommended_stores.isin(relevant_stores)]

# Step 3: Calculate Precision
precision = len(relevant_recommendations) / len(recommended_stores)
print(f"Precision for Content-Based Filtering: {precision:.2f}")

"""# Collaborative Filtering

## Data Preparation

This code prepares the dataset for collaborative filtering by dropping unnecessary columns and resetting the DataFrame index to ensure a clean, sequential index.
"""

m = k.drop(columns=['product_id', 'product_category_name', 'order_id', 'store_name', 'seller_city', 'seller_state',  'review_id'])
m = m.reset_index(drop=True)
m.head()

"""This code defines a function to encode and decode IDs for collaborative filtering:
1. **Define `encode_ids` Function**: The function removes duplicates, encodes unique IDs to integers, and creates a mapping for decoding the integers back to the original IDs.
2. **Encode `customer_id`**: Applies the `encode_ids` function to the `customer_id` column, generating mappings for encoding and decoding customer IDs.
3. **Encode `seller_id`**: Applies the `encode_ids` function to the `seller_id` column, generating mappings for encoding and decoding seller IDs.

"""

def encode_ids(column):
    # Remove duplicates and convert to a list
    unique_ids = column.unique().tolist()
    print('list ID: ', unique_ids)

    # Encoding IDs
    id_to_encoded = {x: i for i, x in enumerate(unique_ids)}
    print('encoded ID: ', id_to_encoded)

    # Decoding numbers to IDs
    encoded_to_id = {i: x for i, x in enumerate(unique_ids)}
    print('encoded number to ID: ', encoded_to_id)

    return id_to_encoded, encoded_to_id

# Usage for user_id
user_to_user_encoded, user_encoded_to_user = encode_ids(m['customer_id'])

# Usage for seller_id
store_to_store_encoded, store_encoded_to_store = encode_ids(m['seller_id'])

"""This code maps the encoded `customer_id` and `seller_id` to new columns in the DataFrame:
1. **Map `customer_id` to `user`**: Replaces the `customer_id` in the DataFrame with its encoded integer value.
2. **Map `seller_id` to `seller`**: Replaces the `seller_id` in the DataFrame with its encoded integer value.

"""

# Mapping dataframe
m['user'] = m['customer_id'].map(user_to_user_encoded)
m['seller'] = m['seller_id'].map(store_to_store_encoded)

"""This code prepares and analyzes the dataset by:
1. **Calculating the Number of Users and Stores**: Counts the unique users and stores in the dataset.
2. **Converting Ratings to Float**: Ensures that the `review_score` column contains float values.
3. **Determining Rating Range**: Identifies the minimum and maximum rating values in the dataset.
4. **Print Summary**: Outputs the number of users, stores, and the range of ratings.
"""

# Get the number of users
num_users = len(user_to_user_encoded)

# Get the number of restaurants
num_store = len(store_encoded_to_store)

# Change rating to float value
m['review_score'] = m['review_score'].values.astype(np.float32)

# Get minimum rating value
min_rating = min(m['review_score'])

# Get maximum rating value
max_rating = max(m['review_score'])

print('Number of User: {}, Number of Resto: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_store, min_rating, max_rating
))

"""## Train Test Split

This code prepares the data for training a collaborative filtering model by:
1. **Shuffling the Data**: Randomly shuffles the DataFrame rows to ensure randomness in training and validation sets.
2. **Creating Feature and Target Variables**: Combines `user` and `seller` into a single feature matrix `x`, and normalizes the `review_score` into the target variable `y`.
3. **Splitting the Data**: Splits the data into 80% for training and 20% for validation.
"""

df = m.sample(frac=1, random_state=42)

# Create a variable x to match user and seller data into one value.
x = df[['user', 'seller']].values

# Create a y variable to create a rating from the results
y = df['review_score'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Split into 80% train data and 20% validation data
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

"""## Training

This code defines a custom neural network model for collaborative filtering:

1. **Class Definition (`RecommenderNet`)**:
   - Inherits from `tf.keras.Model` and is designed to recommend stores to users based on their interaction history.

2. **`__init__` Method**:
   - **Initialize Embedding Layers**: Sets up embedding layers for both users and stores. These layers transform user and store IDs into dense vectors (embeddings).
   - **User and Store Biases**: Adds bias terms for users and stores to capture additional nuances in the data.

3. **`call` Method**:
   - **Embedding Lookup**: Retrieves embeddings for users and stores based on the input IDs.
   - **Dot Product**: Computes the interaction (dot product) between the user and store embeddings, representing how closely related they are.
   - **Add Biases**: Adds the user and store biases to the dot product to refine the prediction.
   - **Sigmoid Activation**: Applies a sigmoid activation function to the output, scaling the predicted rating between 0 and 1.
"""

class RecommenderNet(tf.keras.Model):

  def __init__(self, num_users, num_store, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_store = num_store
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.store_embedding = layers.Embedding( # layer embeddings store
        num_store,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.store_bias = layers.Embedding(num_store, 1) # layer embedding store bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # call embedding layer 1
    user_bias = self.user_bias(inputs[:, 0]) # call embedding layer 2
    store_vector = self.store_embedding(inputs[:, 1]) # call embedding layer 3
    store_bias = self.store_bias(inputs[:, 1]) # call embedding layer 4

    dot_user_store = tf.tensordot(user_vector, store_vector, 2)

    x = dot_user_store + user_bias + store_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""This code trains the collaborative filtering model:

1. **Model Initialization**:
   - Creates an instance of the `RecommenderNet` model with `num_users`, `num_store`, and an embedding size of 50.

2. **Compile the Model**:
   - **Loss Function**: Uses `BinaryCrossentropy` as the loss function to measure prediction error.
   - **Optimizer**: Utilizes the Adam optimizer with a learning rate of 0.001 to update model weights during training.
   - **Metrics**: Tracks the Root Mean Squared Error (RMSE) to evaluate the model's performance.

3. **Model Training**:
   - **Fit the Model**: Trains the model on the training data (`x_train`, `y_train`) for 100 epochs with a batch size of 8.
   - **Validation**: Evaluates the model's performance on the validation data (`x_val`, `y_val`) after each epoch.

"""

model = RecommenderNet(num_users, num_store, 50)

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

"""## Metrics Visualisation

This code visualizes the model's performance over the training epochs.
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""## Getting Recommendation

This code prepares the data for generating store recommendations:

1. **Drop Unnecessary Columns**: Removes columns that are not required for recommendation, such as `product_id`, `product_category_name`, and others.
2. **Group by Store Name**: Aggregates the data by `store_name`, summing up the numerical columns to create a summary for each store.
3. **Reset Index**: Resets the DataFrame index to ensure it is clean and sequential after grouping.
4. **Display Data**: Outputs the first few rows of both the aggregated DataFrame `n` and the original DataFrame `m` for comparison.
"""

n = k.drop(columns=['product_id', 'product_category_name', 'order_id', 'seller_state',  'review_id'])
n = n.groupby('store_name').sum().reset_index()
n.head()

m.head()

"""This code identifies stores visited by a randomly selected user:

1. **Assign DataFrames**: The `seller_df` and `df` variables are assigned to the aggregated and original DataFrames, respectively.
2. **Select a Random User**: Randomly samples one user ID from the `customer_id` column.
3. **Identify Visited Stores**: Filters the DataFrame to find all stores that have been visited by the selected user.

"""

seller_df = n
df = m

# Taking user samples
user_id = df.customer_id.sample(1).iloc[0]
store_visited_by_user = df[df.customer_id == user_id]

"""This code identifies stores that the selected user has not yet visited and prepares the data for recommendations:

1. **Identify Unvisited Stores**:
   - Uses the bitwise operator `~` to filter out stores that the selected user has already visited, leaving only the unvisited ones.

2. **Intersect with Encoded Stores**:
   - Ensures that only stores with encoded IDs are considered by intersecting the unvisited stores with the encoded store IDs.

3. **Prepare Unvisited Store List**:
   - Converts the unvisited store IDs into a list of their encoded representations.

4. **Prepare User-Store Array**:
   - Creates an array combining the encoded user ID with each unvisited store ID, preparing it for input into a recommendation model.

"""

# Operator bitwise (~)
store_not_visited = seller_df[~seller_df['seller_id'].isin(store_visited_by_user.seller_id.values)]['seller_id']
store_not_visited = list(
    set(store_not_visited)
    .intersection(set(store_to_store_encoded.keys()))
)

store_not_visited = [[store_to_store_encoded.get(x)] for x in store_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_store_array = np.hstack(
    ([[user_encoder]] * len(store_not_visited), store_not_visited)
)

"""This code generates and displays store recommendations for the selected user:

1. **Predict Ratings**:
   - Uses the trained model to predict ratings for the unvisited stores by the selected user.
   - Flattens the prediction results into a 1D array.

2. **Identify Top-Rated Stores**:
   - Sorts the predicted ratings in descending order and selects the top 10 store indices.

3. **Map Encoded Store IDs**:
   - Converts the encoded store IDs back to their original `seller_id` values using the mapping dictionary.

4. **Display Top Visited Stores**:
   - Shows the top 5 stores that the user has already visited, ranked by their given review scores.

5. **Display Top 10 Store Recommendations**:
   - Lists the top 10 recommended stores that the user has not visited yet, based on the highest predicted ratings, along with their corresponding cities.

"""

ratings = model.predict(user_store_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_store_ids = [
    store_encoded_to_store.get(store_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Store with high ratings from user')
print('----' * 8)

top_store_user = (
    store_visited_by_user.sort_values(
        by = 'review_score',
        ascending=False
    )
    .head(5)
    .seller_id.values
)

store_df_rows = n[n['seller_id'].isin(top_store_user)]
for row in store_df_rows.itertuples():
    print(row.store_name, ':', row.seller_city)

print('----' * 8)
print('Top 10 store recommendation')
print('----' * 8)

recommended_store = n[n['seller_id'].isin(recommended_store_ids)]
for row in recommended_store.itertuples():
    print(row.store_name, 'in', row.seller_city, 'city')